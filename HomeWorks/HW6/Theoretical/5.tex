\سؤال{\lr{Adaboost}}

\begin{itemize}
	\item الف)
	\item ب)
	خطای تابع نمایی به صورت 
	$E = \Sigma_{n=1}^Ne^{-t_nf_m(x_n)}$
	است که $f_m$ به شکل 
	$f_m(x) = \frac{1}{2}\Sigma_{l = 1}^m\alpha_ly_l(x)$
	تعریف می‌شود و 
	$t_n \in \{-1, 1\}$
	می‌باشد. به‌جای کمینه کردن تابع خطای کلی، می‌توان با توجه به $\alpha_m$ و $y_m(x)$ این کار را انجام داد:
	$$
	E = \Sigma_{n=1}^Ne^{-t_nf_{m - 1}(x_n)-\frac{1}{2}t_n\alpha_my_m(x_n)} = \Sigma_{n = 1}^N w_n^{(m)}e^{-\frac{1}{2}t_n\alpha_my_m(x_n)}
	$$
	که در آن 
	$w_n^{(m)} = e^{-t_nf_{m-1}(x_n)}$
	بوده و آن را می‌توان ثابت در نظر گرفت زیرا تنها $\alpha_m$ و $y_m(x)$ را بهبود می‌دهیم. اگر $\tau_m$ را داده‌هایی که درست دسته‌بندی شده‌اند و $M_m$ را داده‌هایی بگیریم که غلط دسته‌بندی شده‌اند، بگیریم داریم:
	$$
	E = e^{-\frac{\alpha_m}{2}}\Sigma_{n \in \tau_m} w_n^{(m)} + e^{\frac{\alpha_m}{2}} \Sigma_{n \in M_m} w_n^{(m)} 
	$$
	$$
	\implies E = (e^{\frac{\alpha_m}{2}} - e^{-\frac{\alpha_m}{2}})\Sigma_{n = 1}^Nw_n^{(m)}I(y_m(x_n)\neq t_n) + e^{-\frac{\alpha_m}{2}}\Sigma_{n = 1}^Nw_n^{(m)}
	$$
	حال اگر بخواهیم با توجه به $y_m$ کمینه کنیم، عبارت دوم ثابت خواهد بود بنابراین با عنایت به نتایجی که معادلات بالا به دست آمد داریم:
	$$
	w_n^{(m + 1)} = w_n^{(m)} e^{-\frac{1}{2}t_n\alpha_my_m(x_n)}
	$$
	با توجه به 
	$t_ny_m(x_n) 1 - 2I(y_m(x_n) \neq t_n)$ می‌توان عبارت به دست آوردن وزن جدید را به صورت زیر بازنویسی کرد:
	$$
	w_n^{(m + 1)} = w_n^{(m)}e^{-\frac{\alpha_m}{2}}e^{\alpha_mI(y_m(x_n \neq t_n))}
	$$
	به دلیل استقلال 
	$e^{-\frac{\alpha_m}{2}}$ از 
	$n$
	چون همه‌ی عبارات چنین عامل مشترکی دارند می‌توان آن را در نظر نگرفت. 
\end{itemize}