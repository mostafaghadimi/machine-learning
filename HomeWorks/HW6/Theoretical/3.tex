\سؤال{درخت تصمیم}

\begin{itemize}
	\item الف)
	$$
	Gain(y, x_d) = \Sigma_{i}\Sigma_{j} p(y=j, x_d=i) log \frac{p(x_d=i)p(y=j)}{p(y=j, x_d=i)}
	$$
	با توجه به استقلال $y$ و $x_d$ داریم:
	$$
	p(y, x_d) = p(y)p(x_d)
	$$
	$$
	\implies Gain(y, x_d) = \Sigma_{i}\Sigma_{j} p(y=j, x_d=i) log \frac{p(x_d=i)p(y=j)}{p(y=j)p(x_d=i)}
	= \Sigma_{i}\Sigma_{j} p(y=j, x_d=i) log(1) = 0$$ 
	\item ب)
	اگر ملاک ساختن درخت تصمیم‌گیری را براساس مقدار \lr{information gain} قرار دهیم، به دلیل استقلال و یکتا بودن آن (همانند قسمت قبل)، مقدار \lr{gain} برابر با صفر می‌شود. پس به عنوان ریشه‌ی درخت قرار نمی‌گیرد. 
	
	حال اگر فرض کنیم این ویژگی به عنوان ریشه انتخاب شده، \lr{overfitting} به دلیل یکتایی ویژگی‌ها حتمی خواهد بود. برای جلوگیری از \lr{overfitting} عمدتا دو راه‌کار وجود دارد:
	\begin{enumerate}
		\item زودتر متوقف کردن درخت برای  این‌که به نقطه‌ای نرسد که همه‌ی داده‌های آموزش را به‌طور کامل و عالی یاد گرفته باشد.
		\item استفاده از تکنیک \lr{post-pruning}. به روی‌کردی می‌گویند که داده‌ی آموزش و اعتبارسنجی وجود دارد که برای هرس کردن از آن استفاده می‌شود.
	\end{enumerate}
	\item پ)
	\begin{itemize}
		\item یک)
		به دلیل گسسته بودن مقادیر و عدم وجود نویز، هر برگ فقط یک حالت را می‌تواند به خود بگیرد. از هر برگ به ریشه یک مسیر وجود دارد‌ (چون درخت غیرتکراری است.)؛ بنابراین نتیجه‌ای که گرفته می‌شود این است که ویژگی‌های مشترک در دسته‌بندی درست قرار می‌گیرند و درختی وجود دارد که خطای آموزش آن برابر با صفر است.
		\item دو)
		به دلیل پیوسته بودن این کار را نمی‌تواند انجام دهد،‌ زیرا اگر پیوسته باشد خطای آموزش به دلیل تقسیم‌بندی‌های متوالی صفر نمی‌تواند باشد.
		\item سه) زیرا در این جالت مقدار دقیق نداریم (مانند حالت گسسته) و به دلیل پیوستگی دسته‌بندی‌ها را با کمک بازه‌ها انجام می‌دهیم. بنابراین برای این که جوابی داشته باشیم باید مقدار ریشه را داشته باشیم تا بتوان مقدار خطا را کاهش داد.
	\end{itemize}
\end{itemize}