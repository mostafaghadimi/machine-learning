\سؤال{}

همان‌طور که در آخر صفحه ۲۰۶ کتاب \lr{Bishop} توضیح داده است، داریم:

طبق تعریف اگر مجموعه داده، به‌صورت خطی جداپذیر باشد، می‌توان یک $w$ پیدا کرد که برای بعضی از نقاط 
$ w^T\phi(x_n) > 0$
 و برای دیگر نقاط
 $ w^T\phi(x_m) < 0$
 باشد (مقدار آن برابر با صفر نمی‌تواند باشد، چون کلا دو کلاس داریم که بر اساس مثبت یا منفی بودن، دسته‌بندی می‌شوند.).
 
حال دسته‌ی اول را داده‌هایی فرض می‌کنیم که برای آن‌ها رابطه‌ی $ w^T\phi(x_n) > 0$ برقرار است و برای دیگر داده‌ها فرض می‌کنیم که در دسته‌ی دوم هستند (مقدار منفی دارند.).

$$
p(C_1|\phi) = y(\phi) = \sigma(w^T\phi)
$$

حال اگر
$|w| \rightarrow \infty$
باشد، داریم:

$$
p(C_1|\phi(x_n)) = \sigma(w^T\phi(x_n)) \rightarrow 1
$$

با توجه 
$
\begin{cases}
	w^T\phi(x_n) \rightarrow +\infty \\
	w^T\phi(x_m) \rightarrow -\infty
\end{cases}
$

داریم:

$$
p(C_2|\phi(x_m)) = 1 - p(C_1 | \phi(x_m)) = 1 - \sigma(w^T\phi(x_m)) \rightarrow 1
$$

به بیان دیگر برای تابع \lr{likelihood}  اگر 
$|w| \rightarrow \infty$ باشد، همه‌ی داده‌ها به بیش‌ترین مقدارشان یعنی ۱ می‌رسند.

بنابراین برای مجموعه‌ داده‌های خطی جداپذیر، فرآیند یادگیری ممکن است به سمت 
$|w| \rightarrow \infty$
سوق پیدا کند و از \lr{boundary}های خطی برای برچسب‌زدن مجموعه داده استفاده کند که ممکن است باعث \lr{overfitting} شود.