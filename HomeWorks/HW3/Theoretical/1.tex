\سؤال{خطاها}

\begin{enumerate}
	\item 
	\begin{itemize}
		\item \lr{MSE}:
		$$
		L_{MSE}(\theta) = \frac{1}{n}\Sigma_i (x_i - \theta)^2
		$$
		
		$$
		\rightarrow \theta^* = arg max_\theta \frac{1}{n}\Sigma_i (x_i - \theta)^2 = \frac{\partial}{\partial \theta} \frac{1}{n} \Sigma_i (x_i - \theta)^2 = 0
		$$ 
		
		$$
		\rightarrow \theta^* = \frac{2}{n} \Sigma_i (x_i - \theta) = 0 \rightarrow \theta^* = (\frac{2}{n}\Sigma_i x_i) - 2  \theta = 0 
		\rightarrow \theta^* = \frac{\Sigma_i}{n}
		$$
		عبارت به‌دست آمده برای کم‌ترین مقدار میانگین مربعات خطا، میانگین است.
		
		\item \lr{MAE}:
		$$
			L_{MAE}(\theta) = \frac{1}{n}\Sigma_i |x_i - \theta|
		$$
		با توجه به قواعدی که برای مشتق‌گیری از قدرمطلق می‌دانیم اگر $x_i > \theta$ باشد، داریم $x_i - \theta$ و در غیر این‌صورت ($\theta > x_i$)، برابر با $\theta - x_i$ است. 
	حال برای به دست‌آوردن کم‌ترین مقدار خطا، مجموعه‌ای از عبارات داریم که ۱- یا ۱ هستند. بنابراین برای آن که مقدار آن صفر شود باید از نیمی بزرگ‌تر و از نیمی دیگر کوچک‌تر باشد که همان تعریف میانه است.
	\end{itemize}
	\item
		مقایسه \lr{MSE} و \lr{MAE}
	\begin{itemize}
		\item \lr{MSE}:
		\begin{itemize}
			\item مزایا:
				یک راه عالی برای مطمئن شدن از این است که مدل آموزش‌دیده‌ی ما پیش‌بینی‌های \lr{outlier} با مقدار خطاهای بسیار بزرگ ندارد؛ زیرا روش میانگین مربعات خطاها وزن بزرگ‌تری نسبت به مقدار واقعی خطا (به خاطر توان دو در فرمول) اختصاص می‌دهد.
			\item معایب:
			اگر مدل ما یک پیش‌بینی بسیار بد داشته باشد، به دلیل بزرگ‌کردن و اختصاص وزن بیش‌تر از مقدار واقعی خطا به آن باعث وجود عیب در آن می‌شود. زیرا در بسیاری از موردهای عملی فقط دنبال این هستیم که روی اکثریت داده‌ها، مدل خوبی داشته باشیم و توجه زیادی به \lr{outlier}ها ندارند.
			
		\end{itemize}
	
		\item \lr{MAE}:
		مزیت این روش، برطرف کردن مشکل \lr{MSE} و مشکل آن، نداشتن مزیت \lr{MSE} است؛ یعنی:
		\begin{itemize}
			\item مزایا:
			از آن‌جایی که در این مدل در حال محاسبه‌ی قدرمطلق هستیم، مقیاس خطاها تغییری نمی‌کند و خطی باقی می‌ماند.
			
			\item عیب:
			اگر پیش‌بینی‌های \lr{outlier}  برای مدل ما مهم باشد، مدل خوبی نیست؛ زیرا وزن \lr{outlier}ها مشابه با وزن خطاهای کوچک‌تر است که ممکن است در برخی موارد منجر به پیش‌بینی‌های بسیار ضعیفی شود.
		\end{itemize}
	\end{itemize}
	\item توابع اندازه‌گیری خطا \lr{Huber} و \lr{Log-Cosh}
	\begin{itemize}
		\item \lr{Huber} 
		ترکیبی از \lr{MAE} و \lr{MSE} است و فرمول آن در ادامه آورده شده است:
		
		$$
			L_{\delta}(y, f(x)) = \begin{cases}
				\frac{1}{2}(y - f(x)) ^ 2 & for \: |y - f(x)| \leq \delta, \\
				\delta |y - f(x) - \frac{1}{2} \delta^2 & o.w.
			\end{cases}
		$$
		برای توضیح این فرمول می‌توان گفت که برای مقادیری که دلتا کوچک‌تر هستند، از \lr{MSE} و در غیر این صورت، از \lr{MAE} استفاده می‌کند. به بیان دیگر برای خطاهای با اندازه بزرگ از \lr{MAE} و برای خطاهای کوچک از \lr{MSE} بهره می‌برد. در واقع با این کار معایب روش اندازه‌گیری خطا برای هر دو روش قبل را برطرف می‌کند.

		\item \lr{Log-Cosh}: 
		$$
		L_{log-Cosh}(x, \theta) = \Sigma_i log(cosh(\theta - x))
		$$
		
		استفاده از تابع \lr{logarithm} و \lr{cosh} در فرمول این روش اندازه‌گیری خطا، باعث می‌شود که این روش، بسیار مشابه با روش \lr{MSE} باشد با این تفاوت که وقتی پیش‌بینی بسیار بدی داریم، در مقایسه با \lr{MSE}، خیلی روی خطا تاثیرگذار نباشد. (مشکل \lr{MSE} را برطرف می‌کند.)
		
		علاوه‌بر مزیت‌های روش \lr{Huber}، در هر نقطه‌ای دو بار مشتق‌‌پذیر است. 
	\end{itemize}
\end{enumerate}